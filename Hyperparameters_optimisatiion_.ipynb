{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh84wJ0GFX6M",
        "outputId": "11209e18-388b-4656-c703-d16bc41bc6c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEEUKA-rFdg1",
        "outputId": "af77a863-0857-41f1-9f75-1c98bcf718d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 70699777.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 60416804.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 25961722.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 7580791.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "import torch.nn as nn\n",
        "from torch.optim import SGD\n",
        "import optuna\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "\n",
        "# Loading MNIST dataset from torchvision\n",
        "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "\n",
        "#Load the test set (Use the same thing as train_set)\n",
        "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "No_qO8TOI2Qg",
        "outputId": "67cd4563-e5d8-4adf-bed6-ced4616ac131"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-07-31 16:19:44,131] A new study created in memory with name: no-name-02b953f8-716b-40a4-8641-3435d26ac4f5\n",
            "[I 2024-07-31 16:53:46,712] Trial 0 finished with value: 0.9899 and parameters: {'n_kernels1': 63, 'n_kernels2': 71, 'kernel_size1': 6, 'kernel_size2': 5, 'padding1': 0, 'padding2': 1, 'stride1': 2, 'stride2': 2}. Best is trial 0 with value: 0.9899.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD\n",
        "import optuna\n",
        "import numpy as np\n",
        "# Define the model architecture\n",
        "class NN_classification(nn.Module):\n",
        "    def __init__(self, n_kernels1, n_kernels2, kernel_size1, kernel_size2, padding1, padding2, stride1, stride2):\n",
        "        super(NN_classification, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, n_kernels1, kernel_size=kernel_size1, padding=padding1, stride=stride1)\n",
        "        self.conv2 = nn.Conv2d(n_kernels1, n_kernels2, kernel_size=kernel_size2, padding=padding2, stride=stride2)\n",
        "        self.fc1 = nn.Linear(self._get_conv_output((1, 28, 28)), 128)  # Adjust based on the actual output size\n",
        "        self.fc2 = nn.Linear(128, 10)  # 10 classes for MNIST\n",
        "\n",
        "    def _get_conv_output(self, shape):\n",
        "        # Create a dummy tensor with the given shape and pass it through the network\n",
        "        with torch.no_grad():\n",
        "            x = torch.zeros(1, *shape)\n",
        "            x = F.relu(self.conv1(x))\n",
        "            x = F.max_pool2d(x, 2)\n",
        "            x = F.relu(self.conv2(x))\n",
        "            x = F.max_pool2d(x, 2)\n",
        "            return int(np.prod(x.size()))  # Get the flattened size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    n_kernels1 = trial.suggest_int('n_kernels1', 16, 64)\n",
        "    n_kernels2 = trial.suggest_int('n_kernels2', 32, 128)\n",
        "    kernel_size1 = trial.suggest_int('kernel_size1', 3, 7)\n",
        "    kernel_size2 = trial.suggest_int('kernel_size2', 3, 7)\n",
        "    padding1 = trial.suggest_int('padding1', 0, 2)\n",
        "    padding2 = trial.suggest_int('padding2', 0, 2)\n",
        "    stride1 = trial.suggest_int('stride1', 1, 2)\n",
        "    stride2 = trial.suggest_int('stride2', 1, 2)\n",
        "\n",
        "    # Create the model\n",
        "    model = NN_classification(n_kernels1, n_kernels2, kernel_size1, kernel_size2, padding1, padding2, stride1, stride2)\n",
        "\n",
        "    # Define the optimizer and loss function\n",
        "    optimizer = SGD(model.parameters(), lr=0.01)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train the model on the entire training set\n",
        "    num_epochs = 50\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(images)\n",
        "            loss = loss_fn(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_train_loss += loss.item()\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            predictions = model(images)\n",
        "            _, predicted_labels = torch.max(predictions, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted_labels == labels).sum().item()\n",
        "\n",
        "    test_accuracy = correct / total\n",
        "    return test_accuracy\n",
        "\n",
        "# Create the study and optimize\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print('Best hyperparameters: ', study.best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# after finding the best hyperparameters for our model , we are going to re-train our CNN model with these hyperparameters and then evaluate it on a test data."
      ],
      "metadata": {
        "id": "z6eVsNPi9Ld5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 63, kernel_size=6, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(63, 71, kernel_size=5, stride=2, padding=1)\n",
        "        self.fc1 = nn.Linear(71 * 5 * 5, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = x.view(-1, 71 * 5 * 5)  # Corrected view size\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = NeuralNet()\n"
      ],
      "metadata": {
        "id": "XlQ34elK9gat"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from torch.optim import SGD\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "optimizer = SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "\n",
        "        predictions = model(images)\n",
        "\n",
        "\n",
        "        loss = loss_fn(predictions, labels)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print the average loss for the epoch\n",
        "    print(f\"Epoch {epoch+1}, Loss {running_loss/len(train_loader)} \")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5XKgRx1BCmk",
        "outputId": "cb313ef5-228e-409d-8993-56a96476f652"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 0.03894274022167316 \n",
            "Epoch 2, Loss 0.034041424936576206 \n",
            "Epoch 3, Loss 0.03149656269175342 \n",
            "Epoch 4, Loss 0.029612942539187257 \n",
            "Epoch 5, Loss 0.027831871736249122 \n",
            "Epoch 6, Loss 0.026259300231286732 \n",
            "Epoch 7, Loss 0.02492477313891775 \n",
            "Epoch 8, Loss 0.023657638063557 \n",
            "Epoch 9, Loss 0.022444046147764404 \n",
            "Epoch 10, Loss 0.021262303916680346 \n",
            "Model's state_dict:\n",
            "conv1.weight \t torch.Size([63, 1, 6, 6])\n",
            "conv1.bias \t torch.Size([63])\n",
            "conv2.weight \t torch.Size([71, 63, 5, 5])\n",
            "conv2.bias \t torch.Size([71])\n",
            "fc1.weight \t torch.Size([128, 1775])\n",
            "fc1.bias \t torch.Size([128])\n",
            "fc2.weight \t torch.Size([10, 128])\n",
            "fc2.bias \t torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation on the test dataset**"
      ],
      "metadata": {
        "id": "T_-P-Z7iJMLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_loss = 0.0\n",
        "for images, labels in test_loader:\n",
        "    with torch.no_grad():\n",
        "        predictions = model(images)\n",
        "        loss = loss_fn(predictions, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "print(f\"Loss: {test_loss/len(test_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XnJeAK_Imuv",
        "outputId": "5e5dc50b-31a6-4fb7-c601-bc6f91b051a1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.03425671447144086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **here we got a loss function smaller than the one we got when taking random hyperparamters values**"
      ],
      "metadata": {
        "id": "3_I3ukDJJjEa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B6sUY6SWJiGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f3cHum5J32nq"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}