{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIcMuH1IkOVy",
        "outputId": "e26ea1d0-02bb-4cff-f2d7-a63289367570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 6049225.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 159878.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1504972.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4032711.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5),(0.5))\n",
        "])\n",
        "\n",
        "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmq2bDN6r2c_"
      },
      "source": [
        "**# TODO: Figure out how many images are in the train_set and test_set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhhbtiUhqCMo",
        "outputId": "65913018-35e1-471c-df94-9bcd749254d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of images in the training set is: 60000\n",
            "The number of images in the test set is: 10000\n"
          ]
        }
      ],
      "source": [
        "num_train_images = len(train_set)\n",
        "\n",
        "print(f'The number of images in the training set is: {num_train_images}')\n",
        "num_test_images = len(test_set)\n",
        "\n",
        "print(f'The number of images in the test set is: {num_test_images}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNq2jT7KsEm0"
      },
      "source": [
        "# **Building the Neural Network Model **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzBrCOMeru5T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krzHlZAKRByw",
        "outputId": "803a4316-0294-4861-e916-0722d3ff0e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN_classification(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(64, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv4): Conv2d(102, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "## create the NN_classification class\n",
        "class NN_classification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NN_classification, self).__init__()\n",
        "        # initialize layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1, stride=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=1)\n",
        "        self.conv3 = nn.Conv2d(64, 102, kernel_size=3, padding=1, stride=1)\n",
        "        self.conv4 = nn.Conv2d(102, 64, kernel_size=3, padding=1, stride=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 1 * 1, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "        # initialize activation functions\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # define the forward pass\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = self.pool(self.relu(self.conv4(x)))\n",
        "        x = x.view(-1, 64 * 1 * 1)  # Flatten the tensor\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "## create an instance of NN_classification\n",
        "model = NN_classification()\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ej_DHKsEPPd"
      },
      "source": [
        "# **Training the model **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rykc1gwIDg_g",
        "outputId": "c17745c5-0036-4dfd-9c99-828930599acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 2.301031039976108 \n",
            "Epoch 2, Loss 2.2921639566482512 \n",
            "Epoch 3, Loss 1.8344294530496414 \n",
            "Epoch 4, Loss 0.4302783104370652 \n",
            "Epoch 5, Loss 0.18361946780568184 \n",
            "Epoch 6, Loss 0.12466027325730143 \n",
            "Epoch 7, Loss 0.09881782047758336 \n",
            "Epoch 8, Loss 0.08266701888658408 \n",
            "Epoch 9, Loss 0.06996430915597675 \n",
            "Epoch 10, Loss 0.06274524622304893 \n",
            "Model's state_dict:\n",
            "conv1.weight \t torch.Size([32, 1, 3, 3])\n",
            "conv1.bias \t torch.Size([32])\n",
            "conv2.weight \t torch.Size([64, 32, 3, 3])\n",
            "conv2.bias \t torch.Size([64])\n",
            "conv3.weight \t torch.Size([102, 64, 3, 3])\n",
            "conv3.bias \t torch.Size([102])\n",
            "conv4.weight \t torch.Size([64, 102, 3, 3])\n",
            "conv4.bias \t torch.Size([64])\n",
            "fc1.weight \t torch.Size([128, 64])\n",
            "fc1.bias \t torch.Size([128])\n",
            "fc2.weight \t torch.Size([64, 128])\n",
            "fc2.bias \t torch.Size([64])\n",
            "fc3.weight \t torch.Size([10, 64])\n",
            "fc3.bias \t torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from torch.optim import SGD\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "optimizer = SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "\n",
        "        predictions = model(images)\n",
        "\n",
        "\n",
        "        loss = loss_fn(predictions, labels)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print the average loss for the epoch\n",
        "    print(f\"Epoch {epoch+1}, Loss {running_loss/len(train_loader)} \")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2f4YpmmGGZi"
      },
      "source": [
        "# **evaluation on the test_loader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md6ooDrzFD1V",
        "outputId": "2d5df157-323a-44b8-c2e7-5a6ef36bc593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.06096286200351011\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "test_loss = 0.0\n",
        "for images, labels in test_loader:\n",
        "    with torch.no_grad():\n",
        "        predictions = model(images)\n",
        "        loss = loss_fn(predictions, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "print(f\"Loss: {test_loss/len(test_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnXzKMm9gm1W"
      },
      "source": [
        "# **Implementing early stopping**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5DGXcIQi7s8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvku-BMci9KN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "# Split the training set into training and validation sets\n",
        "train_size = int(0.8 * len(train_set))  # 80% training, 20% validation\n",
        "val_size = len(train_set) - train_size\n",
        "train_set, val_set = random_split(train_set, [train_size, val_size])\n",
        "validation_loader = DataLoader(val_set, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ7sfy4DgsIu",
        "outputId": "17b57ab8-0c01-4564-8358-9695522b6305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.05681765765306165, Validation Loss: 0.051730137318372726\n",
            "Epoch 2, Training Loss: 0.05059565552649325, Validation Loss: 0.049796443432569504\n",
            "Epoch 3, Training Loss: 0.04606383941324948, Validation Loss: 0.0036903556901961565\n",
            "Epoch 4, Training Loss: 0.0429450196049202, Validation Loss: 0.009525937028229237\n",
            "Epoch 5, Training Loss: 0.03877751104697994, Validation Loss: 0.005715304519981146\n",
            "Epoch 6, Training Loss: 0.03599408433188313, Validation Loss: 0.0029411781579256058\n",
            "Epoch 7, Training Loss: 0.03330974258097218, Validation Loss: 0.008382689207792282\n",
            "Epoch 8, Training Loss: 0.03114490290428065, Validation Loss: 0.010182598605751991\n",
            "Early stopping triggered!\n",
            "Training is finished!\n"
          ]
        }
      ],
      "source": [
        "# TODO: Complete this code to implement Early stopping\n",
        "patience =5\n",
        "min_delta =0.001\n",
        "best_loss = None\n",
        "patience_counter = 0\n",
        "\n",
        "# Training loop with early stopping\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # Training pass\n",
        "        # Forward pass\n",
        "        predictions = model(images)\n",
        "        loss = loss_fn(predictions, labels)\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # evaluation phase\n",
        "    model.eval()\n",
        "    validation_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in validation_loader:\n",
        "            output = model(images)\n",
        "            validation_loss += loss.item()\n",
        "\n",
        "    # Calculate average losses\n",
        "    training_loss = running_loss / len(train_loader)\n",
        "    validation_loss /= len(validation_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {training_loss}, Validation Loss: {validation_loss}\")\n",
        "\n",
        "    # Early stopping logic\n",
        "    if best_loss is None or validation_loss < best_loss - min_delta:\n",
        "        best_loss = validation_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "print(\"Training is finished!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0JQrOyTm8xY"
      },
      "source": [
        "**patience** refers to the number of epochs to wait for improvement.\n",
        "\n",
        "**min_delta** refers to the minimum change in validation loss to be considered as improvement.\n",
        "\n",
        "By implementing early stopping, the training process stopped at epoch 8. Before applying early stopping, I had initially chosen 10 epochs arbitrarily. The notable difference observed is that with 10 epochs, the validation loss started increasing while the training loss continued to decrease.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDA1wj14oW7i"
      },
      "source": [
        "# **Experimenting with Dropout**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**buliding the neural network**"
      ],
      "metadata": {
        "id": "h2qQEfdTlEyI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPV2GB9Aobrv",
        "outputId": "9022d078-8333-44e6-c145-61f4cdf36e2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jk(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(64, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv4): Conv2d(102, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "class jk(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(jk, self).__init__()\n",
        "        # initialize layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1, stride=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=1)\n",
        "        self.conv3 = nn.Conv2d(64, 102, kernel_size=3, padding=1, stride=1)\n",
        "        self.conv4 = nn.Conv2d(102, 64, kernel_size=3, padding=1, stride=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 1 * 1, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "        # initialize activation functions\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # define the forward pass\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = self.pool(self.relu(self.conv4(x)))\n",
        "        x = x.view(-1, 64 * 1 * 1)  # Flatten the tensor\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model_2 = jk()\n",
        "print(model_2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**training phase**"
      ],
      "metadata": {
        "id": "WC8U-Ha-lMbO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpAJsXboobwv",
        "outputId": "611984e2-6f2f-40b6-c021-100281befe76"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss 2.3055715418573635 \n",
            "Epoch 2, Loss 2.3055976936812086 \n",
            "Epoch 3, Loss 2.305607631008254 \n",
            "Epoch 4, Loss 2.305592506170781 \n",
            "Epoch 5, Loss 2.3055811263859143 \n",
            "Epoch 6, Loss 2.3055870731248023 \n",
            "Epoch 7, Loss 2.3055969329276826 \n",
            "Epoch 8, Loss 2.3055889487012364 \n",
            "Epoch 9, Loss 2.3055902340773073 \n",
            "Epoch 10, Loss 2.305593231085267 \n",
            "Model's state_dict:\n",
            "conv1.weight \t torch.Size([32, 1, 3, 3])\n",
            "conv1.bias \t torch.Size([32])\n",
            "conv2.weight \t torch.Size([64, 32, 3, 3])\n",
            "conv2.bias \t torch.Size([64])\n",
            "conv3.weight \t torch.Size([102, 64, 3, 3])\n",
            "conv3.bias \t torch.Size([102])\n",
            "conv4.weight \t torch.Size([64, 102, 3, 3])\n",
            "conv4.bias \t torch.Size([64])\n",
            "fc1.weight \t torch.Size([128, 64])\n",
            "fc1.bias \t torch.Size([128])\n",
            "fc2.weight \t torch.Size([64, 128])\n",
            "fc2.bias \t torch.Size([64])\n",
            "fc3.weight \t torch.Size([10, 64])\n",
            "fc3.bias \t torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "from torch.optim import SGD\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "optimizer = SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "\n",
        "        predictions = model_2(images)\n",
        "\n",
        "\n",
        "        loss = loss_fn(predictions, labels)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print the average loss for the epoch\n",
        "    print(f\"Epoch {epoch+1}, Loss {running_loss/len(train_loader)} \")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here in this case i think using dropout didn't help to improve the model's performance because the loss function here is greater than the one with no dropout . maybe because the model that i have used is not too complex to use the drop method for optimization.\n",
        "\n",
        "> Ajouter une citation\n",
        "\n"
      ],
      "metadata": {
        "id": "d-RtIIPNDwIm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}